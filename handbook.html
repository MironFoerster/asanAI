<html>
	<!--
		TODO:
		https://codepen.io/ismaelexperiments/pen/gxxjZQ
	-->
	<head>
		<title>asanAI Handbook</title>
		<script src="mathjax/es5/tex-chtml-full.js?config=TeX-AMS-MML_HTMLorMML"></script>

		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
				tex2jax: {
					inlineMath: [['$','$']]
				},
				jax: ["input/TeX","output/CommonHTML"],
				"showMathMenu": true
			});
		</script>
		<script src="tf/tf.js"></script>

		<style>
			body {
				font-family: sans-serif;
			}


			.container{
				margin: 20px auto;
				width:100px;
				height:100px;
				display:grid;
				grid-template-columns: 50px 50px;
				grid-row: auto auto;
				.box{
					color:#fff;
					display:flex;
					align-items:center;
					justify-content:center;
					font-size:40px;
					font-family:sans-serif;
				}

			}

		</style>
	</head>
	<body>
		<img width=64 src="logo.png" />
		<div id="toc"></div>

		<div id="contents">
			<h2>Quickstart</h2>

			<h3>Train on images from webcam</h3>
			TODO

			<h3>Train on images from files</h3>
			TODO

			<h3>Train on CSV</h3>
			TODO

			<h2>General outline</h2>
			asanAI offers a simple way of creating sequential neural networks and train them
			on your own data, from within the browser. It allows you to visualize a lot of
			different intermediate steps. You can, when done training, also <a href="#Export">export</a> the trained
			model to Python and NodeJS.

			<h2>Basic idea of neural networks</h2>

			<h3>Data</h3>
			For neural networks, everything is a tensor. Even if you don't know, you have certainly used tensors. Every
			number, every vector and matrix is a tensor.

			Tensors are a generalization of matrices. Where matrices have 2 dimensions,

			$$
			  \textrm{Second dimension}
			  \stackrel{\mbox{First dimension}}{%
			    \begin{pmatrix}
			    a_{11} & a_{12} & \cdots & a_{1M} \\
			    a_{21} & a_{22} & \cdots & a_{2M} \\
			    \vdots & \vdots & \ddots & \vdots \\
			    a_{N1} & a_{N2} & \cdots & a_{NM}
			    \end{pmatrix}%
			  },
			$$

			tensors have an arbitrary number of dimensions.

			An image, for example, consists of 3 channels, one for red, green and blue,
			each one being a matrix (or submatrix of the image tensor).
			If the image is 2x2 pixels, the image as a tensor would look like this:


			$$
				\text{Image} = \begin{pmatrix}
					\text{Red:} \begin{pmatrix}
						221 & 122 \\
						121 & 11
					\end{pmatrix},
					\text{Green:} \begin{pmatrix}
						0 & 12 \\
						1 & 2
					\end{pmatrix},
					\text{Blue:} \begin{pmatrix}
						225 & 255 \\
						240 & 211
					\end{pmatrix}
				\end{pmatrix}
			$$

			This is a full description of this picture:

			<div class="container">
				<div class="box" style="background-color: #dd00f0"></div>
				<div class="box" style="background-color: #7a0cff"></div>
				<div class="box" style="background-color: #7901f0"></div>
				<div class="box" style="background-color: #0b02d3"></div>
			</div>

			Any data a computer can handle can be expressed as <i>some</i> tensor. They may have more or larger dimensions, but they are
			nonetheless tensors.

			The description of the size of a tensor is called a shape. A two-by-two-Matrix would have the shape [2, 2]. The image above would have
			the shape [2, 2, 3], because its 2x2 pixels and has three channels. An image with 
			64x64 pixel and 3 channels would be [64, 64, 3].

			<h3>Layers</h3>
			Layers act as nested functions. Each layer is a function by itself, and with layers, you put them together into one larger
			function.

			You can imagine them as such:

			$$ \text{Result} = \text{Layer 3}\left(\text{Layer 2}\left(\text{Layer 1}\left(\text{Layer 0}\left(\text{input data}\right)\right)\right)\right) $$

			This is called a sequential model, since the data flows through it sequentially. There are other types of models,
			but they cannot be designed with asanAI.

			<h3>Dimensionality Reduction</h3>
			TODO

			<h3>Training</h3>
			TODO

			<h4>Batch-Size</h4>
			TODO

			<h4>Epochs</h4>
			TODO

			<h3>Predicting</h3>
			TODO

			<h3>Shapes</h3>
			<h4>Input Shape</h4>
			TODO

			<h4>Output Shape</h4>
			TODO

			<h3>Overfitting</h3>
			TODO

			<h2>A very simple neural network</h2>
			TODO

			<h2>Layer Types</h2>
			TODO

			<h3>Basic Layer Types</h3>
			<h4>Dense</h4>

			Available options: <a href="#Trainable">Trainable</a>, <a href="#Use_Bias">Use Bias</a>, <a href="#Units">Units</a>, 
			<a href="#Activation_Functions">Activation Functions</a>, <a href="#Kernel_Initializer">Kernel Initializer</a>, 
			<a href="#Bias_Initializer">Bias Initializer</a>, <a href="#Kernel_Regularizer">Kernel Regularizer</a>, 
			<a href="#Bias_Regularizer">Bias Regularizer</a>, <a href="#Visualize_Layer">Visualize Layer</a>, 
			<a href="#DType">DType</a>

			<br>

			Dense Layers are used as a general-purpose-function-approximator. The basic mathematical structure of a Dense Layer
			is as follows:

			$$
				\text{Dense:} \qquad \underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \underbrace{\begin{pmatrix}
						x_{0}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						-1.4404407739639282
					\end{pmatrix}}_{\mathrm{Kernel^{1 \times 1}}}
					 + \underbrace{\begin{pmatrix}
					0
				\end{pmatrix}}_{\mathrm{Bias}}
			$$

			Depending on the <a href="#Input_Shape">Input Shape</a>, the number of elements in both the Kernel and 
			the Bias may change.

			This, for example, is a Dense Layer with the input shape <tt>[2]</tt>:

			$$
				\text{Dense:} \qquad \underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \underbrace{\begin{pmatrix}
						x_{0}\\
						x_{1}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						0.785955011844635\\
						-0.015428715385496616
					\end{pmatrix}}_{\mathrm{Kernel^{2 \times 1}}}
					 + \underbrace{\begin{pmatrix}
						0.123153419419419
				\end{pmatrix}}_{\mathrm{Bias}}
			$$

			<h4>Flatten</h4>
			Flatten has no options. It creates a simple vector of any matrix.


			Example:

			$$
				\textrm{Flatten}\left( \begin{pmatrix}
					0 & 1 & 2 \\
					3 & 4 & 5 \\
					6 & 7 & 8
				\end{pmatrix}\right) = \left[0 \quad 1 \quad 2 \quad 3 \quad 4 \quad 5 \quad 6 \quad 7 \quad 8 \right]
			$$

			This is used for <a href="#Dimensionality_Reduction">Dimensionality Reduction</a>, in asanAI especially for the transfer
			of image tensors to vectors for Dense Layers (see <a href="#Network_Structures">Network Structures</a>).

			<h4>Dropout</h4>
			Available options: <a href="#Dropout_Rate">Dropout-Rate</a>
			<br>

			The dropout layer sets random values to 0 which a probability given in the Dropout-Rate-option.
			<br>

			$$
				\underbrace{\textrm{Dropout}}_{\text{Dropout-Rate: 50\%}}\left(
					\begin{pmatrix}
						1 & 2 & 3 & 4 \\
						5 & 6 & 7 & 8 \\
						9 & 10 & 11 & 12 \\
						13 & 14 & 15 & 16 \\
						17 & 18 & 19 & 20 \\
						21 & 22 & 23 & 24 \\
					\end{pmatrix}
				\right)
				\xrightarrow{\text{Set values randomly to 0 with a 50\% chance}}
				\begin{pmatrix}
					0 & 0 & 3 & 0 \\
					5 & 6 & 7 & 8 \\
					9 & 10 & 0 & 0 \\
					0 & 0 & 15 & 0 \\
					0 & 18 & 19 & 20 \\
					21 & 0 & 0 & 0 \\
				\end{pmatrix}
			$$
			
			This is only active while training.

			This is used for avoiding <a href="#Overfitting">overfitting</a>.

			<h4>Reshape</h4>
			Available Options: <a href="#Target_Size">Target Size</a>.
			<br>

			This allows incoming data tensors to be reshaped into another tensor. The number of elements does not change, only their
			arragement.

			TODO

			<h3>Activation Layer Types</h3>
			<h4>elu</h4>
			TODO

			$$ 
				\mathrm{elu}\left(x\right) = \left\{
					\begin{array}{ll}
					x & x \geq 0 \\
					\alpha\left(e^x - 1\right)& \, x \lt 0 \\
					\end{array}
				\right.
			$$

			<h4>leakyRelu</h4>
			TODO

			<h4>relu</h4>
			TODO

			<h4>softMax</h4>
			TODO

			<h4>thresholdedRelu</h4>
			TODO

			<h3>Convolutional Layers</h3>
			<h4>conv1d</h4>
			TODO

			<h4>conv2d</h4>
			TODO

			<h4>conv2dTranspose</h4>
			TODO

			<h4>depthwiseConv2d</h4>
			TODO

			<h4>depthwiseConv2d</h4>
			TODO

			<h4>seperableConv2d</h4>
			TODO

			<h4>upsampling2d</h4>
			TODO

			<h3>Pooling layers</h3>
			<h4>averagePooling1d</h4>
			TODO

			<h4>averagePooling2d</h4>
			TODO

			<h4>maxPooling1d</h4>
			TODO

			<h4>maxPooling2d</h4>
			TODO

			<h3>Dropout and noise layers</h3>
			<h4>alphaDropout</h4>
			TODO

			<h4>gaussianDropout</h4>
			TODO

			<h4>gaussianNoise</h4>
			TODO

			<h3>Debug Layers</h3>
			<h4>Debug Layer</h4>
			This layer does not do anything to the data. It just prints them out
			to <tt>console.log</tt>

			<h2>Layer Options</h2>

			<h3>Trainable</h3>
			If enabled, the network's weights and biases are changed while training. If not, they stay the same.

			<h3>Use Bias</h3>
			If enabled, the network has a bias. In Dense Networks, a layer with Use Bias enabled, has this mathematical
			representation:

			$$ 
				\underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \mathrm{\underbrace{LeakyReLU}_{\mathrm{Activation}}}\left(\underbrace{\begin{pmatrix}
						x_{0}\\
						x_{1}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						-1.124836802482605\\
						0.01841479167342186
					\end{pmatrix}}_{\mathrm{Kernel^{2 \times 1}}}
					 + \underbrace{\begin{pmatrix}
						0.123153419419419
					\end{pmatrix}}_{\mathrm{Bias}}
				\right)
			$$

			A Layer without Use Bias enabled would look like this:

			$$
				\underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \mathrm{\underbrace{LeakyReLU}_{\mathrm{Activation}}}\left(\underbrace{\begin{pmatrix}
						x_{0}\\
						x_{1}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						0.24012170732021332\\
						1.188180685043335
					\end{pmatrix}}_{\mathrm{Kernel^{2 \times 1}}}
				\right)
			$$

			The bias allows the function's output to be shifted in any axis.

			<h3>Units</h3>
			TODO

			<h3>DType</h3>
			TODO

			<h3>Standard-Deviation</h3>
			TODO

			<h3>Strides</h3>
			TODO

			<h3>Regularizer</h3>

			<h4>l1</h4>
			TODO

			<h4>l2</h4>
			TODO

			<h4>l1l2</h4>
			TODO

			<h3>Initializers</h3>
			TODO

			<h4>glorotUniform</h4>
			TODO

			<h4>constant</h4>
			TODO

			<h4>glorotNormal</h4>
			TODO

			<h4>heNormal</h4>
			TODO

			<h4>heUniform</h4>
			TODO

			<h4>leCunNormal</h4>
			TODO

			<h4>leCunUniform</h4>
			TODO

			<h4>ones</h4>
			TODO

			<h4>randomNormal</h4>
			TODO

			<h4>randomUniform</h4>
			TODO

			<h4>truncatedNormal</h4>
			TODO

			<h4>varianceScaling</h4>
			TODO

			<h4>zeros</h4>
			TODO

			<h3>Kernel Initializer</h3>
			TODO

			<h3>Bias Initializer</h3>
			TODO

			<h3>Kernel Regularizer</h3>
			TODO

			<h3>Bias Regularizer</h3>
			TODO

			<h3>Dropout Shape</h3>
			TODO

			<h3>Target Shape</h3>
			TODO

			<h2>Initializer Functions</h2>
			TODO

			<h2>Regularizer Functions</h2>
			TODO

			<h2>Loss</h2>
			TODO

			General variables used here:

			$$ \text{Output value of the network} := y_i $$
			$$ \text{Ground truth output value} := \hat{y}_i $$
			$$ \text{Ground truth input value} := \hat{x}_i $$
			$$ \text{Number of elements in total} := n $$

			<h3>meanSquaredError</h3>

			This loss is used when you want to minimize a neural network, where the difference between 2 possible output values has a
			meaningful interpretation. That means, if you care about the exact number coming out of the network, instead of just using it
			as a category id for example.

			$$ \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^n \left(y_i - \hat{y}_i\right)^2 $$

			<h3>binaryCrossentropy</h3>
			TODO

			$$ \text{Binary Crossentropy:} -\frac{1}{n} \sum_{i=1}^n y_i \cdot \log\left(\hat{y}_i\right) + 1\left(-y_i\right) \cdot \log\left(1 - \hat{y}_i\right) $$

			<h3>categoricalCrossentropy</h3>
			TODO

			$$ \text{Categorical Crossentropy:} -\sum_{i=1}^n y_i \log\left(\hat{y}_i\right) $$

			<h3>categoricalHinge</h3>
			TODO

			<h3>hinge</h3>
			TODO

			<h3>meanAbsoluteError</h3>
			TODO

			$$ \mathrm{MAE} = \frac{1}{n} \sum_{i=1}^n \left|y_i - \hat{y}_i\right| $$

			<h3>meanAbsolutePercentageError</h3>
			TODO

			$$ \text{MAPE} = \frac{1}{n} \sum_{t=1}^{n} \left|\frac{\hat{y} - y}{\hat{y}}\right| $$

			<h3>meanSquaredLogarithmicError</h3>
			TODO

			$$ \text{Mean Squared Logarithmic Error:} \frac{1}{n} \sum_{i=0}^n \left(log\left(y_i + 1\right)- \log\left(\hat{y}_i + 1\right)\right)^2 $$

			<h3>poisson</h3>
			TODO

			$$ \text{Poisson:} \frac{1}{n} \sum_{i=0}^n \left(\hat{x}_i - y_i\cdot \log\left(\hat{y}_i\right)\right) $$

			<h3>sparseCategoricalCrossentropy</h3>
			TODO

			<h3>squaredHinge</h3>
			TODO

			$$ \text{Squared Hinge:} \sum_{i=0}^n \left(\mathrm{max}\left(0, 1 - y_i \cdot \hat{y}_i\right)^ 2\right) $$

			<h3>kullbackLeiblerDivergence</h3>
			TODO

			<h3>logcosh</h3>
			TODO

			$$ \text{logcosh:} \sum_{i=0}^n \log(\cosh\left(\hat{y}_i - y_i\right)) $$

			<h2>Metric</h2>
			TODO

			<h3>binaryAccuracy</h3>
			TODO

			<h3>categoricalAccuracy</h3>
			TODO

			<h3>precision</h3>
			TODO

			<h3>categoricalCrossentropy</h3>
			TODO

			<h3>sparseCategoricalCrossentropy</h3>
			TODO

			<h3>meanSquaredError</h3>
			TODO

			<h3>meanAbsoluteError</h3>
			TODO

			<h3>meanAbsolutePercentageError</h3>
			TODO

			<h3>cosine</h3>
			TODO


			<h2>Activation Functions</h2>

			Activation functions are often used to &raquo;squeeze&laquo; the values between a certain range, mostly between
			0 and 1, so that the neural-network-function is nonlinear and as such can approximate nonlinear functions.

			<h3>Linear</h3>

			The linear Activation function is the most simple one. It simply returns the exact same values that are inputted,
			without any changes.

			<h3>Sigmoid</h3>

			The Sigmoid function squeezes all values between 0 and 1, so that large values are near 1 and small values 
			are near 0.

			$$ 
				\mathrm{sigmoid}\left(x\right) = \sigma\left(x\right) = \frac{1}{1+e^{-x}}\qquad (\text{Lower-limit: } 0, \text{Upper-limit: } 1)
			$$

			WHEN TO USE TODO

			<h3>ELU</h3>

			TODO

			$$
				\mathrm{elu}\left(x\right) = \left\{
					\begin{array}{ll}
						x & x \geq 0 \\
						\alpha\left(e^x - 1\right)& \, x \lt 0 \\
					\end{array}
				\right.
			$$

			WHEN TO USE TODO

			<h3>relu6</h3>

			TODO

			$$
			\mathrm{relu6}\left(x\right) = \mathrm{min}\left(\mathrm{max}\left(0, x\right),6\right)\qquad (\text{Lower-limit: } 0, \text{Upper-limit: } 6)
			$$


			WHEN TO USE TODO


			<h3>SeLu</h3>

			TODO

			$$
			\mathrm{selu}\left(x\right) = \mathrm{scale} \cdot \mathrm{elu}\left(x, \alpha\right) = \mathrm{scale} \cdot \left\{
				\begin{array}{ll}
					x & x \geq 0 \\
					\alpha\left(e^x - 1\right)& \, x \lt 0 \\
					\end{array}
				\right.
			$$


			WHEN TO USE TODO
			<h3>SoftPlus</h3>

			TODO

			$$
				\mathrm{softplus}\left(x\right) = \ln\left(1 + e^x\right)
			$$

			WHEN TO USE TODO


			<h3>SoftSign</h3>

			TODO

			$$ \mathrm{softsign}\left(x\right) = \frac{x}{\left(1 + \left| x \right| \right)}\qquad (\text{Lower-limit: } -1, \text{Upper-limit: } 1) $$

			WHEN TO USE TODO

			<h3>SoftMax</h3>

			SoftMax divides each individual item by the whole sum of all items, giving you a percentage of how much each individual value
			is in percentage in relation to the whole.

			$$ \mathrm{softmax}\left(x\right) = \frac{e^{z_j}}{\sum^K_{k=1} e^{z_k}}\qquad (\text{Lower-limit: } 0, \text{Upper-limit: } 1) $$

			SoftMax can be used if you want a percentage of how certain the network is in it's prediction.


			<h3>tanh</h3>

			TODO

			$$ \mathrm{tanh}\left(x\right) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\qquad (\text{Lower-limit: } -1, \text{Upper-limit: } 1) $$

			WHEN TO USE TODO

			<h3>LeakyReLu</h3>
			TODO

			$$ \mathrm{LeakyReLU}\left(x\right) = \mathrm{max}\left(\alpha \cdot x, x\right) $$

			WHEN TO USE 

			<h2>Visualizations</h2>

			<h3>Visualize Layer</h3>
			TODO

			<h3>Math Mode</h3>
			TODO

			<h2>User Modes</h2>

			TODO

			<h3>Beginner-mode</h3>
			TODO

			<h3>Expert-mode</h3>
			TODO

			<h2>Backend</h2>

			<h3>CPU</h3>
			TODO

			<h3>GPU</h3>
			The GPU is faster when you have large GPU memory and a large enough batch size. 

			If the batch size is too small, it may even be slower than CPU.

			<h2>Create your own neural network</h2>

			<h3>From CSV</h3>
			TODO

			<h3>From Images</h3>
			TODO

			<h3>From arbitrary tensors</h3>
			TODO

			<h3>Interpretating the training graph</h3>
			TODO

			<h3>Network Structures</h3>
			TODO

			<h2>Export</h2>

			<h3>Export to Python</h3>
			TODO

			<h3>Export to NodeJS</h3>
			TODO

			<h3>Export to HTML</h3>
			TODO

			<h2>Tips for creating neural networks</h2>
			Start small at first and add until it works.

			<h2>Contact</h2>
			Please contact norman.koch@tu-dresden.de if you have any questions.
		</div>

		<script src="manual.js"></script>
	</body>
</html>

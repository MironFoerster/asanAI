<html>
	<head>
		<title>asanAI Handbook</title>
		<script src="mathjax/es5/tex-chtml-full.js?config=TeX-AMS-MML_HTMLorMML"></script>

		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
				tex2jax: {
					inlineMath: [['$','$']]
				},
				jax: ["input/TeX","output/CommonHTML"],
				"showMathMenu": true
			});
		</script>
	</head>
	<body>
		<div id="toc"></div>

		<div id="contents">
			<h1>asanAI</h1>

			<h2>General outline</h2>
			asanAI offers a simple way of creating sequential neural networks and train them
			on your own data, from within the browser. It allows you to visualize a lot of
			different intermediate steps. You can, when done training, also export the trained
			model to Python and NodeJS.

			<h2>Basic idea of neural networks</h2>
			Neural networks are roughly based on the way the brain works.

			<h3>Data</h3>
			TODO

			<h2>A very simple neural network</h2>
			TODO

			<h2>Layer Types</h2>
			TODO

			<h3>Basic Layer Types</h3>
			<h4>Dense</h4>

			Available options: <a href="#Trainable">Trainable</a>, <a href="#Use_Bias">Use Bias</a>, <a href="#Units">Units</a>, 
			<a href="#Activation_Functions">Activation Functions</a>, <a href="#Kernel_Initializer">Kernel Initializer</a>, 
			<a href="#Bias_Initializer">Bias Initializer</a>, <a href="#Kernel_Regularizer">Kernel Regularizer</a>, 
			<a href="#Bias_Regularizer">Bias Regularizer</a>, <a href="#Visualize_Layer">Visualize Layer</a>, 
			<a href="#DType">DType</a>

			<br>

			Dense Layers are used as a general-purpose-function-approximator. The basic mathematical structure of a Dense Layer
			is as follows:

			$$
				\text{Dense:} \qquad \underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \underbrace{\begin{pmatrix}
						x_{0}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						-1.4404407739639282
					\end{pmatrix}}_{\mathrm{Kernel^{1 \times 1}}}
					 + \underbrace{\begin{pmatrix}
					0
				\end{pmatrix}}_{\mathrm{Bias}}
			$$

			Depending on the <a href="#Input_Shape">Input Shape</a>, the number of elements in both the Kernel and 
			the Bias may change.

			This, for example, is a Dense Layer with the input shape <tt>[2]</tt>:

			$$
				\text{Dense:} \qquad \underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \underbrace{\begin{pmatrix}
						x_{0}\\
						x_{1}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						0.785955011844635\\
						-0.015428715385496616
					\end{pmatrix}}_{\mathrm{Kernel^{2 \times 1}}}
					 + \underbrace{\begin{pmatrix}
						0.123153419419419
				\end{pmatrix}}_{\mathrm{Bias}}
			$$

			<h4>Flatten</h4>
			TODO

			<h4>Dropout</h4>
			TODO

			<h4>Reshape</h4>
			TODO

			<h3>Activation Layer Types</h3>
			<h4>elu</h4>
			TODO

			<h4>leakyRelu</h4>
			TODO

			<h4>relu</h4>
			TODO

			<h4>softMax</h4>
			TODO

			<h4>thresholdedRelu</h4>
			TODO

			<h3>Convolutional Layers</h3>
			<h4>conv1d</h4>
			TODO

			<h4>conv2d</h4>
			TODO

			<h4>conv2dTranspose</h4>
			TODO

			<h4>depthwiseConv2d</h4>
			TODO

			<h4>depthwiseConv2d</h4>
			TODO

			<h4>seperableConv2d</h4>
			TODO

			<h4>upsampling2d</h4>
			TODO

			<h3>Pooling layers</h3>
			<h4>averagePooling1d</h4>
			TODO

			<h4>averagePooling2d</h4>
			TODO

			<h4>maxPooling1d</h4>
			TODO

			<h4>maxPooling2d</h4>
			TODO

			<h3>Dropout and noise layers</h3>
			<h4>alphaDropout</h4>
			TODO

			<h4>gaussianDropout</h4>
			TODO

			<h4>gaussianNoise</h4>
			TODO

			<h3>Debug Layers</h3>
			<h4>Debug Layer</h4>
			This layer does not do anything to the data. It just prints them out
			to <tt>console.log</tt>

			<h2>Layer Options</h2>

			<h3>Trainable</h3>
			If enabled, the network's weights and biases are changed while training. If not, they stay the same.

			<h3>Use Bias</h3>
			If enabled, the network has a bias. In Dense Networks, a layer with Use Bias enabled, has this mathematical
			representation:

			$$ 
				\underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \mathrm{\underbrace{LeakyReLU}_{\mathrm{Activation}}}\left(\underbrace{\begin{pmatrix}
						x_{0}\\
						x_{1}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						-1.124836802482605\\
						0.01841479167342186
					\end{pmatrix}}_{\mathrm{Kernel^{2 \times 1}}}
					 + \underbrace{\begin{pmatrix}
						0.123153419419419
					\end{pmatrix}}_{\mathrm{Bias}}
				\right)
			$$

			A Layer without Use Bias enabled would look like this:

			$$
				\underbrace{\begin{pmatrix}
					y_{0}
					\end{pmatrix}}_{\mathrm{Output}}
					 = \mathrm{\underbrace{LeakyReLU}_{\mathrm{Activation}}}\left(\underbrace{\begin{pmatrix}
						x_{0}\\
						x_{1}
					\end{pmatrix}}_{\mathrm{Input}}
					 \times \underbrace{\begin{pmatrix}
						0.24012170732021332\\
						1.188180685043335
					\end{pmatrix}}_{\mathrm{Kernel^{2 \times 1}}}
				\right)
			$$

			The bias allows the function to be shifted in any axis.

			<h3>Units</h3>
			TODO

			<h3>Dtype</h3>
			TODO

			<h3>Standard-Deviation</h3>
			TODO

			<h3>Strides</h3>
			TODO

			<h3>Kernel Initializer</h3>
			TODO

			<h3>Bias Initializer</h3>
			TODO

			<h3>Kernel Regularizer</h3>
			TODO

			<h3>Bias Regularizer</h3>
			TODO

			<h2>Tips for creating neural networks</h2>
			Start small at first and add until it works.

			<h2>Activation Functions</h2>

			Activation functions are often used to &raquo;squeeze&laquo; the values between a certain range, mostly between
			0 and 1, so that the neural-network-function is nonlinear and as such can approximate nonlinear functions.

			<h3>Linear</h3>

			The linear Activation function is the most simple one. It simply returns the exact same values that are inputted,
			without any changes.

			<h3>Sigmoid</h3>

			The Sigmoid function squeezes all values between 0 and 1, so that large values are near 1 and small values 
			are near 0.

			$$ 
				\mathrm{sigmoid}\left(x\right) = \sigma\left(x\right) = \frac{1}{1+e^{-x}}\qquad (\text{Lower-limit: } 0, \text{Upper-limit: } 1)
			$$

			WHEN TO USE TODO

			<h3>ELU</h3>

			TODO

			$$
				\mathrm{elu}\left(x\right) = \left\{
					\begin{array}{ll}
						x & x \geq 0 \\
						\alpha\left(e^x - 1\right)& \, x \lt 0 \\
					\end{array}
				\right.
			$$

			WHEN TO USE TODO

			<h3>relu6</h3>

			TODO

			$$
			\mathrm{relu6}\left(x\right) = \mathrm{min}\left(\mathrm{max}\left(0, x\right),6\right)\qquad (\text{Lower-limit: } 0, \text{Upper-limit: } 6)
			$$


			WHEN TO USE TODO


			<h3>SeLu</h3>

			TODO

			$$
			\mathrm{selu}\left(x\right) = \mathrm{scale} \cdot \mathrm{elu}\left(x, \alpha\right) = \mathrm{scale} \cdot \left\{
				\begin{array}{ll}
					x & x \geq 0 \\
					\alpha\left(e^x - 1\right)& \, x \lt 0 \\
					\end{array}
				\right.
			$$


			WHEN TO USE TODO
			<h3>SoftPlus</h3>

			TODO

			$$
				\mathrm{softplus}\left(x\right) = \ln\left(1 + e^x\right)
			$$

			WHEN TO USE TODO


			<h3>SoftSign</h3>

			TODO

			$$ \mathrm{softsign}\left(x\right) = \frac{x}{\left(1 + \left| x \right| \right)}\qquad (\text{Lower-limit: } -1, \text{Upper-limit: } 1) $$

			WHEN TO USE TODO

			<h3>SoftMax</h3>

			SoftMax divides each individual item by the whole sum of all items, giving you a percentage of how much each individual value
			is in percentage in relation to the whole.

			$$ \mathrm{softmax}\left(x\right) = \frac{e^{z_j}}{\sum^K_{k=1} e^{z_k}}\qquad (\text{Lower-limit: } 0, \text{Upper-limit: } 1) $$

			SoftMax can be used if you want a percentage of how certain the network is in it's prediction.


			<h3>tanh</h3>

			TODO

			$$ \mathrm{tanh}\left(x\right) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\qquad (\text{Lower-limit: } -1, \text{Upper-limit: } 1) $$

			WHEN TO USE TODO

			<h3>LeakyReLu</h3>
			TODO

			$$ \mathrm{LeakyReLU}\left(x\right) = \mathrm{max}\left(\alpha \cdot x, x\right) $$

			WHEN TO USE 

			<h2>Visualizations</h2>
			<h3>Visualize Layer</h3>

			<h2>Contact</h2>
			Please contact norman.koch@tu-dresden.de if you have any questions.
		</div>

		<script>
			function toc () {
				var toc = "";
				var level = 0;

				document.getElementById("contents").innerHTML =
					document.getElementById("contents").innerHTML.replace(
						/<h([\d])>([^<]+)<\/h([\d])>/gi,
						function (str, openLevel, titleText, closeLevel) {
							if (openLevel != closeLevel) {
								return str;
							}

							if (openLevel > level) {
								toc += (new Array(openLevel - level + 1)).join("<ul>");
							} else if (openLevel < level) {
								toc += (new Array(level - openLevel + 1)).join("</ul>");
							}

							level = parseInt(openLevel);

							var anchor = titleText.replace(/ /g, "_");
							toc += "<li><a href=\"#" + anchor + "\">" + titleText
								+ "</a></li>";

							return "<h" + openLevel + "><a name=\"" + anchor + "\">"
								+ titleText + "</a></h" + closeLevel + ">";
						}
					);

				if (level) {
					toc += (new Array(level + 1)).join("</ul>");
				}

				document.getElementById("toc").innerHTML += toc;
			};

			toc();
		</script>
	</body>
</html>
